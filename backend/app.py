"""HR Process Mining Tool - Backend (v5)"""
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
app = FastAPI(title="HR Process Mining v5")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

try:
    from .schemas import ReviewRequest, ChatRequest, ValidateL7Request, ContextualSuggestRequest
    from .llm_service import check_llm, call_llm, close_http_client, get_llm_debug_status
    from .chat_orchestrator import orchestrate_chat, get_chain_status
    from .prompt_templates import REVIEW_SYSTEM, COACH_TEMPLATE, CONTEXTUAL_SUGGEST_SYSTEM, FIRST_SHAPE_SYSTEM, PDD_ANALYSIS, PDD_INSIGHTS_SYSTEM
    from .flow_services import describe_flow, mock_review, mock_validate
except ImportError:
    from schemas import ReviewRequest, ChatRequest, ValidateL7Request, ContextualSuggestRequest
    from llm_service import check_llm, call_llm, close_http_client, get_llm_debug_status
    from chat_orchestrator import orchestrate_chat, get_chain_status
    from prompt_templates import REVIEW_SYSTEM, COACH_TEMPLATE, CONTEXTUAL_SUGGEST_SYSTEM, FIRST_SHAPE_SYSTEM, PDD_ANALYSIS, PDD_INSIGHTS_SYSTEM
    from flow_services import describe_flow, mock_review, mock_validate


@app.post("/api/review")
async def review_flow(req: ReviewRequest):
    fd = describe_flow(req.currentNodes, req.currentEdges)
    r = await call_llm(REVIEW_SYSTEM, f"ì»¨í…ìŠ¤íŠ¸: {req.context}\ní”Œë¡œìš°:\n{fd}",
                       max_tokens=1200, temperature=0.3)
    return r or mock_review(req.currentNodes, req.currentEdges)


@app.post("/api/pdd-insights")
async def pdd_insights(req: ReviewRequest):
    fd = describe_flow(req.currentNodes, req.currentEdges)
    r = await call_llm(PDD_INSIGHTS_SYSTEM, f"ì»¨í…ìŠ¤íŠ¸: {req.context}\ní”Œë¡œìš°:\n{fd}")
    return r or {"summary": "ë¶„ì„ì— ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.", "inefficiencies": [], "digitalWorker": [], "sscCandidates": [], "redesign": []}


@app.post("/api/chat")
async def chat(req: ChatRequest):
    try:
        fd = describe_flow(req.currentNodes, req.currentEdges)
        history_lines = []
        for t in req.recentTurns[-4:]:
            role = "ì‚¬ìš©ì" if t.get("role") == "user" else "ì½”ì¹˜"
            content = str(t.get("content", "")).strip()
            if content:
                history_lines.append(f"- {role}: {content}")
        history_block = "\n".join(history_lines) if history_lines else "(ì—†ìŒ)"
        summary = req.conversationSummary or "(ì—†ìŒ)"
        prompt = (
            f"ì»¨í…ìŠ¤íŠ¸: {req.context}\n"
            f"í”Œë¡œìš°:\n{fd}\n"
            f"ëŒ€í™” ìš”ì•½: {summary}\n"
            f"ìµœê·¼ ëŒ€í™” 4í„´:\n{history_block}\n"
            f"ì§ˆë¬¸: {req.message}"
        )
        return await orchestrate_chat(COACH_TEMPLATE, prompt, req.message, req.currentNodes, req.currentEdges)
    except Exception:
        logger.exception("/api/chat ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ")
        return {"speech": "ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", "suggestions": [], "quickQueries": []}


@app.post("/api/validate-l7")
async def validate_l7(req: ValidateL7Request):
    # Phase 1: ì‹¤ì‹œê°„ L7 íŒì •ì€ í”„ë¡ íŠ¸ ë£° ì—”ì§„ì—ì„œ ì²˜ë¦¬.
    # ë°±ì—”ë“œ validate-l7ëŠ” ì €ì¥/ë°°ì¹˜/í˜¸í™˜ ìš©ë„ë¡œ ë£° ê¸°ë°˜ ê²°ê³¼ë§Œ ë°˜í™˜.
    return mock_validate(req.label, req.nodeType, llm_failed=False)


@app.post("/api/contextual-suggest")
async def contextual_suggest(req: ContextualSuggestRequest):
    fd = describe_flow(req.currentNodes, req.currentEdges)
    r = await call_llm(CONTEXTUAL_SUGGEST_SYSTEM, f"ì»¨í…ìŠ¤íŠ¸: {req.context}\ní”Œë¡œìš°:\n{fd}")
    return r or {"guidance": "", "quickQueries": []}


@app.post("/api/first-shape-welcome")
async def first_shape_welcome(req: ContextualSuggestRequest):
    process_name = req.context.get("processName", "HR í”„ë¡œì„¸ìŠ¤")
    process_type = req.context.get("l5", "í”„ë¡œì„¸ìŠ¤")
    r = await call_llm(FIRST_SHAPE_SYSTEM, f"í”„ë¡œì„¸ìŠ¤ëª…: {process_name}\ní”„ë¡œì„¸ìŠ¤ íƒ€ì…: {process_type}\n\nì‚¬ìš©ìê°€ ì´ í”„ë¡œì„¸ìŠ¤ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. í™˜ì˜í•˜ê³  ê²©ë ¤í•´ì£¼ì„¸ìš”.")

    if r:
        return {
            "text": f"ğŸ‘‹ {r.get('greeting', '')}\n\n{r.get('processFlowExample', '')}\n\n{r.get('guidanceText', '')}",
            "quickQueries": r.get("quickQueries", []),
        }
    return {
        "text": f"ğŸ‘‹ ì²« ë‹¨ê³„ê°€ ì¶”ê°€ë˜ì—ˆë„¤ìš”! \"{process_name}\" í”„ë¡œì„¸ìŠ¤ë¥¼ í•¨ê»˜ ì™„ì„±í•´ë³´ê² ìŠµë‹ˆë‹¤.\n\në‹¤ìŒ ë‹¨ê³„ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì•„ë˜ ì§ˆë¬¸ìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ êµ¬ì¡°ë¥¼ ìƒê°í•´ë³´ì„¸ìš”.",
        "quickQueries": ["ì¼ë°˜ì ì¸ ë‹¨ê³„ëŠ” ë­ê°€ ìˆë‚˜ìš”?", "ì–´ë–¤ ë¶„ê¸°ì ì´ í•„ìš”í• ê¹Œìš”?", "ì´ í”„ë¡œì„¸ìŠ¤ì˜ ì£¼ìš” ì—­í• ì€ ëˆ„êµ¬ì¸ê°€ìš”?"],
    }


@app.post("/api/analyze-pdd")
async def analyze_pdd(req: ReviewRequest):
    fd = describe_flow(req.currentNodes, req.currentEdges)
    r = await call_llm(PDD_ANALYSIS, f"ì»¨í…ìŠ¤íŠ¸: {req.context}\ní”Œë¡œìš°:\n{fd}")
    if r:
        return r
    recs = []
    for n in req.currentNodes:
        if n.type in ("start", "end"):
            continue
        cat = "as_is"
        if any(k in n.label for k in ["ì¡°íšŒ", "ì…ë ¥", "ì¶”ì¶œ", "ì§‘ê³„"]):
            cat = "digital_worker"
        elif any(k in n.label for k in ["í†µë³´", "ì•ˆë‚´", "ë°œì†¡"]):
            cat = "ssc_transfer"
        recs.append({"nodeId": n.id, "nodeLabel": n.label, "suggestedCategory": cat, "reason": "ê·œì¹™ ê¸°ë°˜", "confidence": "low"})
    return {"recommendations": recs, "summary": "ê·œì¹™ ê¸°ë°˜ ìë™ ë¶„ë¥˜ì…ë‹ˆë‹¤."}


@app.get("/api/health")
async def health():
    llm = await check_llm()
    return {
        "status": "ok",
        "version": "5.0",
        "llm_connected": llm,
        "mode": "live" if llm else "mock",
        "llm_debug": get_llm_debug_status(),
        "chat_chain": get_chain_status(),
    }


@app.on_event("shutdown")
async def shutdown():
    await close_http_client()


if __name__ == "__main__":
    import uvicorn

    try:
        uvicorn.run(app, host="0.0.0.0", port=8000)
    except SystemExit:
        logger.warning("Port 8000 is busy. Trying port 8002...")
        uvicorn.run(app, host="0.0.0.0", port=8002)
