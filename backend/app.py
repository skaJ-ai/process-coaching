"""HR Process Mining Tool - Backend (v5)"""
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
app = FastAPI(title="Process Coaching AI ë² íƒ€ë²„ì „")

# Import CORS configuration
try:
    from .env_config import ALLOWED_ORIGINS
except ImportError:
    from env_config import ALLOWED_ORIGINS

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)


@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.exception(f"Unhandled exception on {request.url.path}")
    return JSONResponse(
        status_code=500,
        content={"speech": "ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", "suggestions": [], "quickQueries": []},
    )

try:
    from .schemas import ReviewRequest, ChatRequest, ValidateL7Request, ContextualSuggestRequest, CategorizeNodesRequest
    from .llm_service import check_llm, call_llm, close_http_client, get_llm_debug_status
    from .chat_orchestrator import orchestrate_chat, get_chain_status, _classify_intent
    from .prompt_templates import REVIEW_SYSTEM, COACH_TEMPLATE, CONTEXTUAL_SUGGEST_SYSTEM, FIRST_SHAPE_SYSTEM, PDD_ANALYSIS, PDD_INSIGHTS_SYSTEM, KNOWLEDGE_PROMPT, CATEGORIZE_PROMPT
    from .flow_services import describe_flow, mock_review, mock_validate
    from .l345_reference import get_l345_context
except ImportError:
    from schemas import ReviewRequest, ChatRequest, ValidateL7Request, ContextualSuggestRequest, CategorizeNodesRequest
    from llm_service import check_llm, call_llm, close_http_client, get_llm_debug_status
    from chat_orchestrator import orchestrate_chat, get_chain_status, _classify_intent
    from prompt_templates import REVIEW_SYSTEM, COACH_TEMPLATE, CONTEXTUAL_SUGGEST_SYSTEM, FIRST_SHAPE_SYSTEM, PDD_ANALYSIS, PDD_INSIGHTS_SYSTEM, KNOWLEDGE_PROMPT, CATEGORIZE_PROMPT
    from flow_services import describe_flow, mock_review, mock_validate
    from l345_reference import get_l345_context


def _build_l345_block(context: dict) -> str:
    """req.contextì—ì„œ L345 ì°¸ì¡° ë¸”ë¡ ìƒì„±. ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´."""
    if not isinstance(context, dict):
        return ""
    return get_l345_context(
        context.get("l4", ""),
        context.get("l5", ""),
        context.get("processName", ""),
    )


@app.post("/api/review")
async def review_flow(req: ReviewRequest):
    fd = describe_flow(req.currentNodes, req.currentEdges)
    l345 = _build_l345_block(req.context) if isinstance(req.context, dict) else ""
    ctx_block = (
        f"[í”„ë¡œì„¸ìŠ¤ ì»¨í…ìŠ¤íŠ¸]\n"
        f"L4: {req.context.get('l4', 'ë¯¸ì„¤ì •') if isinstance(req.context, dict) else req.context}\n"
        f"L5: {req.context.get('l5', 'ë¯¸ì„¤ì •') if isinstance(req.context, dict) else ''}\n"
        f"L6(í™œë™): {req.context.get('processName', 'ë¯¸ì„¤ì •') if isinstance(req.context, dict) else ''}\n"
    )
    if l345:
        ctx_block += f"\n{l345}\n"
    r = await call_llm(REVIEW_SYSTEM, f"{ctx_block}\ní”Œë¡œìš°:\n{fd}",
                       max_tokens=1200, temperature=0.3)
    return r or mock_review(req.currentNodes, req.currentEdges)


@app.post("/api/pdd-insights")
async def pdd_insights(req: ReviewRequest):
    fd = describe_flow(req.currentNodes, req.currentEdges)
    l345 = _build_l345_block(req.context) if isinstance(req.context, dict) else ""
    pdd_ctx = f"ì»¨í…ìŠ¤íŠ¸: {req.context}\n"
    if l345:
        pdd_ctx += f"\n{l345}\n"
    r = await call_llm(PDD_INSIGHTS_SYSTEM, f"{pdd_ctx}í”Œë¡œìš°:\n{fd}")
    return r or {"summary": "ë¶„ì„ì— ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.", "inefficiencies": [], "digitalWorker": [], "sscCandidates": [], "redesign": []}


@app.post("/api/chat")
async def chat(req: ChatRequest):
    try:
        intent = _classify_intent(req.message)
        history_lines = []
        for t in req.recentTurns[-10:]:
            role = "ì‚¬ìš©ì" if t.get("role") == "user" else "ì½”ì¹˜"
            content = str(t.get("content", "")).strip()
            if content:
                history_lines.append(f"- {role}: {content}")
        history_block = "\n".join(history_lines) if history_lines else "(ì—†ìŒ)"
        summary = req.conversationSummary or "(ì—†ìŒ)"

        l345 = _build_l345_block(req.context) if isinstance(req.context, dict) else ""
        ctx_lines = (
            f"[í”„ë¡œì„¸ìŠ¤ ì»¨í…ìŠ¤íŠ¸]\n"
            f"L4: {req.context.get('l4', 'ë¯¸ì„¤ì •') if isinstance(req.context, dict) else req.context}\n"
            f"L5: {req.context.get('l5', 'ë¯¸ì„¤ì •') if isinstance(req.context, dict) else ''}\n"
            f"L6(í™œë™): {req.context.get('processName', 'ë¯¸ì„¤ì •') if isinstance(req.context, dict) else ''}\n"
        )
        if l345:
            ctx_lines += f"\n{l345}\n"

        if intent == "knowledge":
            # ì§€ì‹ ì§ˆë¬¸: í”Œë¡œìš° ìƒì„¸ ìƒëµ, ë…¸ë“œ ìˆ˜ë§Œ ì „ë‹¬í•˜ì—¬ í† í° ì ˆì•½
            node_count = len(req.currentNodes)
            prompt = (
                f"{ctx_lines}\n"
                f"í˜„ì¬ í”Œë¡œìš°: ë…¸ë“œ {node_count}ê°œ\n"
                f"ìµœê·¼ ëŒ€í™”:\n{history_block}\n"
                f"ì§ˆë¬¸: {req.message}"
            )
        else:
            fd = describe_flow(req.currentNodes, req.currentEdges)
            prompt = (
                f"{ctx_lines}\n"
                f"í”Œë¡œìš°:\n{fd}\n"
                f"ëŒ€í™” ìš”ì•½: {summary}\n"
                f"ìµœê·¼ ëŒ€í™”:\n{history_block}\n"
                f"ì§ˆë¬¸: {req.message}"
            )
        return await orchestrate_chat(COACH_TEMPLATE, prompt, req.message, req.currentNodes, req.currentEdges)
    except Exception:
        logger.exception("/api/chat ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ")
        return {"speech": "ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", "suggestions": [], "quickQueries": []}


@app.post("/api/validate-l7")
async def validate_l7(req: ValidateL7Request):
    # Phase 1: ì‹¤ì‹œê°„ L7 íŒì •ì€ í”„ë¡ íŠ¸ ë£° ì—”ì§„ì—ì„œ ì²˜ë¦¬.
    # ë°±ì—”ë“œ validate-l7ëŠ” ì €ì¥/ë°°ì¹˜/í˜¸í™˜ ìš©ë„ë¡œ ë£° ê¸°ë°˜ ê²°ê³¼ë§Œ ë°˜í™˜.
    return mock_validate(req.label, req.nodeType, llm_failed=False)


@app.post("/api/contextual-suggest")
async def contextual_suggest(req: ContextualSuggestRequest):
    # ì´ˆê¸° ê°€ì´ë“œìš©ì´ë¯€ë¡œ ìš”ì•½ ëª¨ë“œë¡œ í† í° ì ˆì•½
    fd = describe_flow(req.currentNodes, req.currentEdges, summary=True)
    r = await call_llm(CONTEXTUAL_SUGGEST_SYSTEM, f"ì»¨í…ìŠ¤íŠ¸: {req.context}\ní”Œë¡œìš°:\n{fd}")
    return r or {"guidance": "", "quickQueries": []}


@app.post("/api/first-shape-welcome")
async def first_shape_welcome(req: ContextualSuggestRequest):
    process_name = req.context.get("processName", "HR í”„ë¡œì„¸ìŠ¤")
    process_type = req.context.get("l5", "í”„ë¡œì„¸ìŠ¤")
    l345 = _build_l345_block(req.context) if isinstance(req.context, dict) else ""
    welcome_prompt = f"í”„ë¡œì„¸ìŠ¤ëª…: {process_name}\ní”„ë¡œì„¸ìŠ¤ íƒ€ì…: {process_type}\n"
    if l345:
        welcome_prompt += f"\n{l345}\n"
    welcome_prompt += "\nì‚¬ìš©ìê°€ ì´ í”„ë¡œì„¸ìŠ¤ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. í™˜ì˜í•˜ê³  ê²©ë ¤í•´ì£¼ì„¸ìš”."
    r = await call_llm(FIRST_SHAPE_SYSTEM, welcome_prompt)

    if r:
        return {
            "text": f"ğŸ‘‹ {r.get('greeting', '')}\n\n{r.get('processFlowExample', '')}\n\n{r.get('guidanceText', '')}",
            "quickQueries": r.get("quickQueries", []),
        }
    return {
        "text": f"ğŸ‘‹ ì²« ë‹¨ê³„ê°€ ì¶”ê°€ë˜ì—ˆë„¤ìš”! \"{process_name}\" í”„ë¡œì„¸ìŠ¤ë¥¼ í•¨ê»˜ ì™„ì„±í•´ë³´ê² ìŠµë‹ˆë‹¤.\n\në‹¤ìŒ ë‹¨ê³„ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì•„ë˜ ì§ˆë¬¸ìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ êµ¬ì¡°ë¥¼ ìƒê°í•´ë³´ì„¸ìš”.",
        "quickQueries": ["ì¼ë°˜ì ì¸ ë‹¨ê³„ëŠ” ë­ê°€ ìˆë‚˜ìš”?", "ì–´ë–¤ ë¶„ê¸°ì ì´ í•„ìš”í• ê¹Œìš”?", "ì´ í”„ë¡œì„¸ìŠ¤ì˜ ì£¼ìš” ì—­í• ì€ ëˆ„êµ¬ì¸ê°€ìš”?"],
    }


@app.post("/api/analyze-pdd")
async def analyze_pdd(req: ReviewRequest):
    fd = describe_flow(req.currentNodes, req.currentEdges)
    r = await call_llm(PDD_ANALYSIS, f"ì»¨í…ìŠ¤íŠ¸: {req.context}\ní”Œë¡œìš°:\n{fd}")
    if r:
        return r
    recs = []
    for n in req.currentNodes:
        if n.type in ("start", "end"):
            continue
        cat = "as_is"
        if any(k in n.label for k in ["ì¡°íšŒ", "ì…ë ¥", "ì¶”ì¶œ", "ì§‘ê³„"]):
            cat = "digital_worker"
        elif any(k in n.label for k in ["í†µë³´", "ì•ˆë‚´", "ë°œì†¡"]):
            cat = "ssc_transfer"
        recs.append({"nodeId": n.id, "nodeLabel": n.label, "suggestedCategory": cat, "reason": "ê·œì¹™ ê¸°ë°˜", "confidence": "low"})
    return {"recommendations": recs, "summary": "ê·œì¹™ ê¸°ë°˜ ìë™ ë¶„ë¥˜ì…ë‹ˆë‹¤."}


@app.post("/api/categorize-nodes")
async def categorize_nodes(req: CategorizeNodesRequest):
    """ZBR ê¸°ì¤€ìœ¼ë¡œ ë…¸ë“œì˜ ì¹´í…Œê³ ë¦¬ ì¶”ì²œ (TO-BE ëª¨ë“œ ì „ìš©)"""
    # Prepare node descriptions
    node_descriptions = []
    for n in req.nodes:
        if n.type in ("start", "end"):
            continue
        desc = f"- {n.label} (ID: {n.id}, íƒ€ì…: {n.type})"
        if n.systemName:
            desc += f", ì‹œìŠ¤í…œ: {n.systemName}"
        if n.duration:
            desc += f", ì†Œìš”ì‹œê°„: {n.duration}"
        node_descriptions.append(desc)

    if not node_descriptions:
        return []

    prompt = f"""í”„ë¡œì„¸ìŠ¤: {req.context.get('processName', 'Unknown')}
L4 ëª¨ë“ˆ: {req.context.get('l4', 'Unknown')}
L5 ë‹¨ìœ„ì—…ë¬´: {req.context.get('l5', 'Unknown')}

[ë¶„ë¥˜ ëŒ€ìƒ ë…¸ë“œ ëª©ë¡]
{chr(10).join(node_descriptions)}

ìœ„ ë…¸ë“œë“¤ì„ ZBR 4ê°€ì§€ ì§ˆë¬¸ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³  JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”."""

    result = await call_llm(CATEGORIZE_PROMPT, prompt)

    # Fallback: ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜
    if not result:
        fallback = []
        for n in req.nodes:
            if n.type in ("start", "end"):
                continue
            cat = "as_is"
            reasoning = "LLM ì‹¤íŒ¨ë¡œ ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜"

            if any(k in n.label for k in ["ì¡°íšŒ", "ì…ë ¥", "ì¶”ì¶œ", "ì§‘ê³„", "ê³„ì‚°", "ì „ì†¡"]):
                cat = "digital_worker"
                reasoning = "ë°ì´í„° ì²˜ë¦¬ ì‘ì—…ìœ¼ë¡œ ìë™í™” ê°€ëŠ¥"
            elif any(k in n.label for k in ["í†µë³´", "ì•ˆë‚´", "ë°œì†¡", "ì ‘ìˆ˜", "ì •ì‚°"]):
                cat = "ssc_transfer"
                reasoning = "í‘œì¤€í™” ê°€ëŠ¥í•œ ê³µí†µ ì—…ë¬´"
            elif any(k in n.label for k in ["í™•ì¸", "ê²€í† "]) and "ìŠ¹ì¸" not in n.label:
                cat = "delete_target"
                reasoning = "í˜•ì‹ì  í™•ì¸ ë‹¨ê³„ë¡œ í†µí•© ë˜ëŠ” ì œê±° ê²€í† "

            fallback.append({
                "nodeId": n.id,
                "suggestedCategory": cat,
                "confidence": "low",
                "reasoning": reasoning
            })
        return fallback

    return result


@app.get("/api/health")
async def health():
    llm = await check_llm()
    return {
        "status": "ok",
        "version": "5.0",
        "llm_connected": llm,
        "mode": "live" if llm else "mock",
        "llm_debug": get_llm_debug_status(),
        "chat_chain": get_chain_status(),
    }


@app.on_event("shutdown")
async def shutdown():
    await close_http_client()


if __name__ == "__main__":
    import uvicorn

    try:
        uvicorn.run(app, host="0.0.0.0", port=8000)
    except SystemExit:
        logger.warning("Port 8000 is busy. Trying port 8002...")
        uvicorn.run(app, host="0.0.0.0", port=8002)
