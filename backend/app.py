"""HR Process Mining Tool - Backend (v5)"""
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional
import httpx, json, os, logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
app = FastAPI(title="HR Process Mining v5")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

LLM_BASE_URL = os.getenv("LLM_BASE_URL", "http://10.240.248.157:8533/v1")
LLM_MODEL = os.getenv("LLM_MODEL", "Qwen3-Next")
# LLM_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai"  # Gemini OpenAI compatible endpoint
# LLM_MODEL = "gemini-1.5-flash"
GEMINI_API_KEY = "AIzaSyBDxyMb9qgsiiCTQfmlm7CZFpCn6h4JOZc"
USE_MOCK = os.getenv("USE_MOCK", "auto")

class FlowNode(BaseModel):
    id: str; type: str; label: str; position: dict = Field(default_factory=lambda:{"x":0,"y":0})
    inputLabel: Optional[str] = None; outputLabel: Optional[str] = None; systemName: Optional[str] = None
    duration: Optional[str] = None; category: Optional[str] = None; swimLaneId: Optional[str] = None
class FlowEdge(BaseModel):
    id: str; source: str; target: str; label: Optional[str] = None
    sourceHandle: Optional[str] = None; targetHandle: Optional[str] = None
class ReviewRequest(BaseModel):
    currentNodes: list[FlowNode]; currentEdges: list[FlowEdge]; userMessage: str = ""; context: dict
class ChatRequest(BaseModel):
    message: str; context: dict; currentNodes: list[FlowNode] = []; currentEdges: list[FlowEdge] = []
class ValidateL7Request(BaseModel):
    nodeId: str; label: str; nodeType: str; context: dict; currentNodes: list[FlowNode] = []; currentEdges: list[FlowEdge] = []

class ContextualSuggestRequest(BaseModel):
    context: dict; currentNodes: list[FlowNode] = []; currentEdges: list[FlowEdge] = []

# Collaborative Coaching Tone Guidelines
COACHING_TONE = """
[ì–´ì¡° ì›ì¹™]
- ì œì•ˆí˜• í‘œí˜„ ì‚¬ìš©: "ê³ ë ¤í•´ ë³´ì„¸ìš”", "~í•˜ë©´ ì–´ë–¨ê¹Œìš”?", "~í•  ìˆ˜ ìˆì–´ìš”"
- ì ˆëŒ€ì  í‘œí˜„ íšŒí”¼: "ë°˜ë“œì‹œ", "ê¸ˆì§€", "must" ëŒ€ì‹  "ê¶Œì¥í•©ë‹ˆë‹¤", "ë” ëª…í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
- ê³µê° í‘œí˜„ í¬í•¨: "ì´í•´í•©ë‹ˆë‹¤", "ë³µì¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
- ì´ìœ  ì„¤ëª…: ëª¨ë“  ì œì•ˆì— "ì™œ ì¤‘ìš”í•œì§€", "ì–´ë–¤ ì´ì ì´ ìˆëŠ”ì§€" í¬í•¨
- ì¡°ê±´í˜• ì–¸ì–´: "~í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤", "~ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤"
- ì§ˆë¬¸í˜• ì œì•ˆ: ê°€ëŠ¥í•œ ê²½ìš° "~í•˜ëŠ” ê²ƒì€ ì–´ë–¨ê¹Œìš”?"
"""

L7_GUIDE = """[L7 ì‘ì„± ì›ì¹™]
ì œ3ìê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡:
- ëª…í™•í•œ ì£¼ì–´ì™€ ëª©ì ì–´ í¬í•¨ì„ ê¶Œì¥í•©ë‹ˆë‹¤
- í•˜ë‚˜ì˜ í™”ë©´ ë‚´ ì—°ì† ë™ì‘ì€ 1ê°œ L7ë¡œ í‘œí˜„í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤
- íŒë‹¨ ì‹œ ëª…í™•í•œ ê¸°ì¤€ê°’ì„ í¬í•¨í•˜ë©´ ì˜ì‚¬ê²°ì •ì´ ëª…í™•í•´ì§‘ë‹ˆë‹¤

[ê¶Œì¥ ë™ì‚¬]
ì¡°íšŒí•œë‹¤, ì…ë ¥í•œë‹¤, ìˆ˜ì •í•œë‹¤, ì €ì¥í•œë‹¤, ì¶”ì¶œí•œë‹¤, ë¹„êµí•œë‹¤, ì§‘ê³„í•œë‹¤,
ê¸°ë¡í•œë‹¤, ì²¨ë¶€í•œë‹¤, íŒì •í•œë‹¤, ìŠ¹ì¸í•œë‹¤, ë°˜ë ¤í•œë‹¤, ê²°ì •í•œë‹¤,
ì˜ˆì™¸ë¡œ ì²˜ë¦¬í•œë‹¤, ìš”ì²­í•œë‹¤, ì¬ìš”ì²­í•œë‹¤, ì•ˆë‚´í•œë‹¤, ê³µì§€í•œë‹¤, ì—ìŠ¤ì»¬ë ˆì´ì…˜í•œë‹¤

[êµ¬ì²´í™”ê°€ í•„ìš”í•œ ë™ì‚¬]
ì²˜ë¦¬í•œë‹¤, ì§„í–‰í•œë‹¤, ê´€ë¦¬í•œë‹¤, ëŒ€ì‘í•œë‹¤, ì§€ì›í•œë‹¤, ê°œì„ í•œë‹¤, ìµœì í™”í•œë‹¤,
ê²€í† í•œë‹¤, í™•ì¸í•œë‹¤, ì •ë¦¬í•œë‹¤, ê³µìœ í•œë‹¤, ì¡°ìœ¨í•œë‹¤, í˜‘ì˜í•œë‹¤, ë°˜ì˜í•œë‹¤
â†’ ì´ëŸ¬í•œ ë™ì‚¬ëŠ” êµ¬ì²´ì ì¸ í–‰ìœ„ë¡œ ë°”ê¾¸ë©´ ë” ëª…í™•í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤

ì°¸ê³ : ì‹œìŠ¤í…œëª…ì€ ë¼ë²¨ì´ ì•„ë‹Œ ë…¸ë“œ ë©”íƒ€ë°ì´í„°ë¡œ ê´€ë¦¬í•˜ë©´ ê¹”ë”í•©ë‹ˆë‹¤."""

def describe_flow(nodes, edges):
    if not nodes: return "í”Œë¡œìš° ë¹„ì–´ìˆìŒ."

    # â”€â”€â”€ Flow Statistics â”€â”€â”€
    node_types = {"start": 0, "end": 0, "process": 0, "decision": 0, "subprocess": 0}
    for n in nodes:
        if hasattr(n, 'data'):
            node_types[n.data.get('nodeType', 'process')] += 1
        elif hasattr(n, 'nodeType'):
            node_types[getattr(n, 'nodeType', 'process')] += 1
        else:
            node_types['process'] += 1

    total_nodes = len(nodes)
    total_edges = len(edges)
    has_swim_lanes = any(getattr(n, 'swimLaneId', None) or (hasattr(n, 'data') and n.data.get('swimLaneId')) for n in nodes)

    # â”€â”€â”€ Phase Detection â”€â”€â”€
    if total_nodes <= 2:
        phase = "ì´ˆê¸° ë‹¨ê³„"
    elif total_nodes <= 5 or not any(n_id for n_id, e in [(e['source'], e) for e in edges]
                                      for tgt in [e['target'] for e in edges]
                                      if node_types.get('end', 0) == 0):
        phase = "ì§„í–‰ ì¤‘"
    else:
        phase = "ì™„ì„± ë‹¨ê³„"

    # â”€â”€â”€ Structural Analysis â”€â”€â”€
    all_node_ids = {n.id if hasattr(n, 'id') else getattr(n, 'id', None) for n in nodes}
    source_ids = {e['source'] if isinstance(e, dict) else e.source for e in edges}
    target_ids = {e['target'] if isinstance(e, dict) else e.target for e in edges}

    orphan_count = len(all_node_ids - source_ids - target_ids)
    orphan_nodes = list(all_node_ids - source_ids - target_ids)

    has_start = node_types.get('start', 0) > 0
    has_end = node_types.get('end', 0) > 0
    start_connected = any(e.get('source') if isinstance(e, dict) else e.source
                         for n in nodes
                         if (getattr(n, 'nodeType', None) or (hasattr(n, 'data') and n.data.get('nodeType'))) == 'start'
                         for e in edges)

    disconnected_ends = [(n.id if hasattr(n, 'id') else None) for n in nodes
                         if (getattr(n, 'nodeType', None) or (hasattr(n, 'data') and n.data.get('nodeType'))) == 'end'
                         and (n.id if hasattr(n, 'id') else None) not in target_ids]

    # â”€â”€â”€ HR Process Checkpoints â”€â”€â”€
    hr_keywords = {'ìŠ¹ì¸': 0, 'ê²°ì¬': 0, 'ì˜ˆì™¸': 0, 'ê²€í† ': 0, 'íŒì •': 0, 'ìš”ì²­': 0}
    for n in nodes:
        label = getattr(n, 'label', '') or (n.data.get('label', '') if hasattr(n, 'data') else '')
        for kw in hr_keywords:
            if kw in label:
                hr_keywords[kw] += 1

    has_hr_checkpoints = any(v > 0 for v in hr_keywords.values())
    hr_coverage = ", ".join([f"{kw}({v}ê±´)" for kw, v in hr_keywords.items() if v > 0]) or "ì—†ìŒ"

    # â”€â”€â”€ Generate Rich Description â”€â”€â”€
    lines = [
        f"[í”Œë¡œìš° í†µê³„] ì´ {total_nodes}ê°œ ë…¸ë“œ, {total_edges}ê°œ ì—°ê²°",
        f"  êµ¬ì„±: ì‹œì‘({node_types['start']}) > íƒœìŠ¤í¬({node_types['process']}) / ë¶„ê¸°({node_types['decision']}) / í•˜ìœ„ê³µì •({node_types['subprocess']}) > ì¢…ë£Œ({node_types['end']})",
        f"  ìˆ˜ì˜ë ˆì¸: {'ì‚¬ìš© ì¤‘' if has_swim_lanes else 'ë¯¸ì‚¬ìš©'}",
        f"[ì§„í–‰ë„] {phase}",
        f"[êµ¬ì¡° ìƒíƒœ] ì‹œì‘({has_start}), ì¢…ë£Œ({has_end}), ê³ ì•„({orphan_count}), ì—°ê²°ìœ¨({100*total_edges//max(total_nodes-1,1)}%)",
    ]

    if orphan_count > 0:
        lines.append(f"  âš  {orphan_count}ê°œ ì—°ê²°ì•ˆë¨: {orphan_nodes}")
    if not has_end:
        lines.append(f"  âš  ì¢…ë£Œ ë…¸ë“œ ì—†ìŒ")
    if disconnected_ends:
        lines.append(f"  âš  {len(disconnected_ends)}ê°œ ì¢…ë£Œ ë…¸ë“œ ì—°ê²° ì•ˆë¨")

    lines.append(f"[HR í”„ë¡œì„¸ìŠ¤ ìš”ì†Œ] {hr_coverage}")
    lines.append("")
    lines.append("ë…¸ë“œ ëª©ë¡:")

    for n in nodes:
        node_id = n.id if hasattr(n, 'id') else getattr(n, 'id', '?')
        node_type = getattr(n, 'nodeType', None) or (n.data.get('nodeType') if hasattr(n, 'data') else 'process')
        label = getattr(n, 'label', '') or (n.data.get('label', '') if hasattr(n, 'data') else '')
        t = {"process":"íƒœìŠ¤í¬","decision":"ë¶„ê¸°","subprocess":"í•˜ìœ„ê³µì •","start":"ì‹œì‘","end":"ì¢…ë£Œ"}.get(node_type, node_type)

        meta = ""
        if hasattr(n, 'systemName') and n.systemName:
            meta += f" [SYS:{n.systemName}]"
        elif hasattr(n, 'data') and n.data.get('systemName'):
            meta += f" [SYS:{n.data.get('systemName')}]"

        if hasattr(n, 'duration') and n.duration:
            meta += f" [â±{n.duration}]"
        elif hasattr(n, 'data') and n.data.get('duration'):
            meta += f" [â±{n.data.get('duration')}]"

        category = getattr(n, 'category', None) or (n.data.get('category') if hasattr(n, 'data') else None)
        if category and category != "as_is":
            meta += f" <{category}>"

        swimlane = getattr(n, 'swimLaneId', None) or (n.data.get('swimLaneId') if hasattr(n, 'data') else None)
        if swimlane:
            meta += f" [ë ˆì¸:{swimlane}]"

        lines.append(f"  [{node_id}] ({t}) {label}{meta}")

    lines.append("ì—°ê²° êµ¬ì¡°:")
    for e in edges:
        source = e['source'] if isinstance(e, dict) else e.source
        target = e['target'] if isinstance(e, dict) else e.target
        label = e.get('label', '') if isinstance(e, dict) else (e.label if hasattr(e, 'label') else '')
        lines.append(f"  {source} â†’ {target}{f' [{label}]' if label else ''}")

    return "\n".join(lines)

_llm_available: Optional[bool] = None
async def check_llm():
    global _llm_available
    if USE_MOCK == "true": _llm_available = False; return False
    if USE_MOCK == "false": _llm_available = True; return True
    if _llm_available is not None: return _llm_available
    try:
        headers = {"Authorization": f"Bearer {GEMINI_API_KEY}"} if "googleapis.com" in LLM_BASE_URL else None
        async with httpx.AsyncClient(timeout=5.0) as c: 
            r = await c.get(f"{LLM_BASE_URL}/models", headers=headers)
            if r.status_code != 200: logger.error(f"LLM check failed: {r.status_code} {r.text}")
            _llm_available = r.status_code == 200
    except Exception as e: logger.error(f"LLM check error: {e}"); _llm_available = False
    return _llm_available

async def call_llm(system_prompt, user_message):
    if not await check_llm(): return None
    try:
        headers = {"Authorization": f"Bearer {GEMINI_API_KEY}"} if "googleapis.com" in LLM_BASE_URL else None
        async with httpx.AsyncClient(timeout=None) as c:
            r = await c.post(f"{LLM_BASE_URL}/chat/completions", json={"model": LLM_MODEL, "messages": [{"role":"system","content":system_prompt},{"role":"user","content":user_message}], "temperature": 0.7, "max_tokens": 2000}, headers=headers)
            r.raise_for_status(); content = r.json()["choices"][0]["message"]["content"]
            if "<think>" in content: content = content.split("</think>")[-1]
            if "```json" in content: content = content.split("```json")[1].split("```")[0]
            elif "```" in content: content = content.split("```")[1].split("```")[0]
            return json.loads(content.strip())
    except Exception as e: logger.error(f"LLM error: {e}"); return None

REVIEW_SYSTEM = f"""ë‹¹ì‹ ì€ HR í”„ë¡œì„¸ìŠ¤ ì„¤ê³„ë¥¼ ë•ëŠ” í˜‘ë ¥ì  ì½”ì¹˜ì…ë‹ˆë‹¤.

{COACHING_TONE}
{L7_GUIDE}

ì—­í• : í”Œë¡œìš°ë¥¼ ë¶„ì„í•˜ê³  ê°œì„  ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ëª…ë ¹ì´ ì•„ë‹Œ ì œì•ˆìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”.

ì‘ë‹µ í˜•ì‹ (JSON):
{{
  "speech": "ë¶„ì„ ê²°ê³¼ë¥¼ ì¹œê·¼í•˜ê²Œ ìš”ì•½ (ì˜ˆ: 'ì¢‹ì€ ì‹œì‘ì…ë‹ˆë‹¤! ëª‡ ê°€ì§€ ê³ ë ¤ì‚¬í•­ì„ ê³µìœ ë“œë¦´ê²Œìš”')",
  "suggestions": [
    {{
      "action": "ADD|MODIFY|DELETE",
      "summary": "ì œì•ˆ ë‚´ìš©",
      "reason": "ì™œ ì´ê²ƒì´ ë„ì›€ì´ ë˜ëŠ”ì§€ êµ¬ì²´ì  ì´ìœ . '~í•˜ë©´ ë” ëª…í™•í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤' í˜•íƒœ",
      "confidence": "high|medium|low",
      ...
    }}
  ],
  "quickQueries": ["í›„ì† ì§ˆë¬¸1", "í›„ì† ì§ˆë¬¸2"]
}}

ì¤‘ìš”: ëª¨ë“  ì œì•ˆì€ ì œì•ˆí˜• ì–´ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš” (ì˜ˆ: "ì¶”ê°€í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”", "ê³ ë ¤í•´ ë³´ì‹œê² ì–´ìš”?").
"""

COACH_TEMPLATE = f"""ë‹¹ì‹ ì€ HR í”„ë¡œì„¸ìŠ¤ ì„¤ê³„ë¥¼ í•¨ê»˜ ë§Œë“¤ì–´ê°€ëŠ” ì½”ì¹˜ì…ë‹ˆë‹¤.

{COACHING_TONE}
{L7_GUIDE}

ì—­í• : ì‚¬ìš©ì ì§ˆë¬¸ì— ê³µê°í•˜ë©° ë‹µë³€í•˜ê³ , êµ¬ì²´ì  ê°œì„  ë°©í–¥ì„ ì œì•ˆí•©ë‹ˆë‹¤.

ì‘ë‹µ í˜•ì‹ (JSON):
{{
  "speech": "ê³µê°í•˜ë©° ë‹µë³€ (ì˜ˆ: 'ì¢‹ì€ ì§ˆë¬¸ì…ë‹ˆë‹¤. ì´ëŸ° ê´€ì ì—ì„œ ìƒê°í•´ë³¼ ìˆ˜ ìˆì–´ìš”')",
  "suggestions": [...],
  "quickQueries": ["ë‹¤ìŒìœ¼ë¡œ í™•ì¸í•  ì§ˆë¬¸2~3ê°œ"]
}}

ì¤‘ìš”:
- ëª¨ë“  ë¬¸ì¥ì„ ì œì•ˆí˜•ìœ¼ë¡œ ("~í•˜ë©´ ì–´ë–¨ê¹Œìš”?", "~ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”")
- ë¶€ì •ì  í‘œí˜„ íšŒí”¼ ("ë¬¸ì œ", "í‹€ë ¸ë‹¤" ëŒ€ì‹  "ê°œì„  ê¸°íšŒ", "ë” ë‚˜ì€ ë°©ë²•")
"""

L7_VALIDATE = f"""ë‹¹ì‹ ì€ L7 ì‘ì„±ì„ ë•ëŠ” í’ˆì§ˆ ì½”ì¹˜ì…ë‹ˆë‹¤.

{COACHING_TONE}
{L7_GUIDE}

ì—­í• : L7 ë¼ë²¨ì„ ê²€í† í•˜ê³  ê°œì„  ë°©í–¥ì„ ì œì•ˆí•©ë‹ˆë‹¤. ë¹„íŒì´ ì•„ë‹Œ ì½”ì¹­ìœ¼ë¡œ ì ‘ê·¼í•˜ì„¸ìš”.

ì‘ë‹µ í˜•ì‹ (JSON):
{{
  "pass": true/false,
  "score": 0-100,
  "confidence": "high|medium|low",
  "issues": [
    {{
      "ruleId": "R-XX",
      "severity": "reject|warning",
      "friendlyTag": "ê°„ë‹¨í•œ íƒœê·¸ (ì˜ˆ: 'êµ¬ì²´í™” ê¶Œì¥')",
      "message": "ì œì•ˆí˜• ë©”ì‹œì§€ (ì˜ˆ: 'ë” êµ¬ì²´ì ì¸ ë™ì‚¬ë¥¼ ì‚¬ìš©í•˜ë©´ ëª…í™•í•´ì§ˆ ìˆ˜ ìˆì–´ìš”')",
      "suggestion": "ê°œì„  ë°©í–¥",
      "reasoning": "ì™œ ì´ ê°œì„ ì´ ë„ì›€ë˜ëŠ”ì§€"
    }}
  ],
  "rewriteSuggestion": "ê°œì„ ëœ ë¼ë²¨ ì œì•ˆ",
  "encouragement": "ê¸ì •ì  í”¼ë“œë°± (ì˜ˆ: 'ì¢‹ì€ ë°©í–¥ì…ë‹ˆë‹¤! ì¡°ê¸ˆë§Œ ë” êµ¬ì²´í™”í•˜ë©´ ì™„ë²½í•´ìš”')"
}}

ì¤‘ìš”: "ê¸ˆì§€", "í‹€ë ¸ë‹¤" ê°™ì€ ë¶€ì • í‘œí˜„ ê¸ˆì§€. í•­ìƒ ê°œì„ ì˜ ì´ìœ ì™€ ì´ì  ì„¤ëª….
"""

CONTEXTUAL_SUGGEST_SYSTEM = f"""ë‹¹ì‹ ì€ ì¡°ìš©íˆ ì§€ì¼œë³´ë‹¤ê°€ í•„ìš”í•œ ìˆœê°„ í•œë§ˆë”” ê±´ë„¤ëŠ” ì‚¬ë ¤ê¹Šì€ ì½”ì¹˜ì…ë‹ˆë‹¤.

{COACHING_TONE}
{L7_GUIDE}

ì—­í• : í˜„ì¬ í”Œë¡œìš°ë¥¼ ë³´ê³  ë¹ ì§„ ë‹¨ê³„ë‚˜ ì˜ˆì™¸ë¥¼ ì§§ê³  ë¶€ë“œëŸ½ê²Œ ì§šì–´ì¤ë‹ˆë‹¤.

ì‘ë‹µ í˜•ì‹ (JSONë§Œ):
{{
  "guidance": "í•œ ì¤„ ì œì•ˆ (ì˜ˆ: 'ì˜ˆì™¸ ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ì¶”ê°€í•˜ë©´ ë” ì™„ë²½í•´ì§ˆ ê²ƒ ê°™ì•„ìš”')",
  "tone": "gentle",
  "quickQueries": ["ê¶ê¸ˆí• ë§Œí•œ ì§ˆë¬¸1", "ì§ˆë¬¸2"]
}}

ì¤‘ìš”: ë„ˆë¬´ ë¹ˆë²ˆí•˜ê±°ë‚˜ ê°•ì••ì ì´ì§€ ì•Šê²Œ. ì‘ì—… ì¤‘ë‹¨ì„ ìµœì†Œí™”.
"""

FIRST_SHAPE_SYSTEM = f"""ë‹¹ì‹ ì€ HR í”„ë¡œì„¸ìŠ¤ ì„¤ê³„ë¥¼ ì²˜ìŒ ì‹œì‘í•˜ëŠ” ì‚¬ìš©ìë¥¼ í™˜ì˜í•˜ê³  ê²©ë ¤í•˜ëŠ” ì¹œì ˆí•œ ì½”ì¹˜ì…ë‹ˆë‹¤.

{COACHING_TONE}

ì—­í• : ì²« ë²ˆì§¸ í”„ë¡œì„¸ìŠ¤ ë‹¨ê³„ë¥¼ ì¶”ê°€í•œ ì‚¬ìš©ìì—ê²Œ:
1. ë”°ëœ»í•œ í™˜ì˜ ì¸ì‚¬
2. í•´ë‹¹ í”„ë¡œì„¸ìŠ¤ì˜ ì¼ë°˜ì ì¸ íë¦„ì„ ì œì‹œ
3. ê³ ë ¤í•  ì‚¬í•­(ì˜ˆ: ì˜ˆì™¸ ì²˜ë¦¬, ìŠ¹ì¸ ë¶„ê¸°)
4. ë‹¤ìŒ ë‹¨ê³„ì— ëŒ€í•œ í¬ê´„ì ì¸ ì œì•ˆ

ì‘ë‹µ í˜•ì‹ (JSON):
{{
  "greeting": "í™˜ì˜ ì¸ì‚¬ (ì˜ˆ: 'ì¢‹ì€ ì‹œì‘ì…ë‹ˆë‹¤! í•¨ê»˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ì™„ì„±í•´ë³´ê² ìŠµë‹ˆë‹¤')",
  "processFlowExample": "ì¼ë°˜ì ì¸ í”„ë¡œì„¸ìŠ¤ íë¦„ (â†’ë¡œ ë‹¨ê³„ë¥¼ ì—°ê²°)",
  "guidanceText": "ì´ í”„ë¡œì„¸ìŠ¤ì—ì„œ ê³ ë ¤í•  ì ë“¤ì„ í¬í•¨í•œ ì¹œì ˆí•œ ì„¤ëª… (2-3ë¬¸ì¥)",
  "quickQueries": ["í›„ì† ì§ˆë¬¸1", "í›„ì† ì§ˆë¬¸2", "í›„ì† ì§ˆë¬¸3"]
}}

ì¤‘ìš”: ëª¨ë“  í‘œí˜„ì„ ì œì•ˆí˜• ì–´ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""

PDD_ANALYSIS = """ë‹¹ì‹ ì€ HR í”„ë¡œì„¸ìŠ¤ ìë™í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê° íƒœìŠ¤í¬ë¥¼ ë¶„ì„í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì²œí•˜ì„¸ìš”.\nì‘ë‹µ(JSONë§Œ): {"recommendations":[{"nodeId":"...","nodeLabel":"...","suggestedCategory":"...","reason":"...","confidence":"high|medium|low"}],"summary":"ì „ì²´ ìš”ì•½"}"""


def mock_validate(label):
    """Mock L7 validation with gentle, suggestive feedback"""
    need_specificity = ["ì²˜ë¦¬í•œë‹¤","ì§„í–‰í•œë‹¤","ê´€ë¦¬í•œë‹¤","í™•ì¸í•œë‹¤","ê²€í† í•œë‹¤"]
    issues = []

    for v in need_specificity:
        if v in label:
            issues.append({
                "ruleId": "R-03",
                "severity": "warning",
                "friendlyTag": "êµ¬ì²´í™” ê¶Œì¥",
                "message": f"'{v}' ëŒ€ì‹  ë” êµ¬ì²´ì ì¸ ë™ì‚¬ë¥¼ ì‚¬ìš©í•˜ë©´ ëª…í™•í•´ì§ˆ ìˆ˜ ìˆì–´ìš”",
                "suggestion": "ì˜ˆ: ì¡°íšŒí•œë‹¤, ì…ë ¥í•œë‹¤, ì €ì¥í•œë‹¤, ìŠ¹ì¸í•œë‹¤ ë“±",
                "reasoning": "êµ¬ì²´ì  ë™ì‚¬ëŠ” ì œ3ìê°€ ì •í™•íˆ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤"
            })

    if not label.strip().endswith("ë‹¤") and not label.strip().endswith("ë‹¤."):
        issues.append({
            "ruleId": "R-15",
            "severity": "warning",
            "friendlyTag": "í‘œì¤€ í˜•ì‹",
            "message": "'~í•œë‹¤' í˜•íƒœë¡œ ë§ˆë¬´ë¦¬í•˜ë©´ ì¼ê´€ì„±ì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤",
            "suggestion": "ë™ì‚¬í˜• ì–´ë¯¸ ì‚¬ìš©ì„ ê¶Œì¥ë“œë ¤ìš”",
            "reasoning": "í‘œì¤€ í˜•ì‹ì€ í”Œë¡œìš° ì „ì²´ì˜ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤"
        })

    has_critical = len([i for i in issues if i["severity"] == "reject"]) == 0
    score = 90 if has_critical and not issues else 70 if has_critical else 50
    encouragement = "ì˜ ì‘ì„±í•˜ì…¨ì–´ìš”!" if not issues else "ì¢‹ì€ ì‹œì‘ì…ë‹ˆë‹¤. ì¡°ê¸ˆë§Œ ë” ë‹¤ë“¬ìœ¼ë©´ ì™„ë²½í•´ìš”!"

    return {
        "pass": has_critical,
        "score": score,
        "confidence": "medium",
        "issues": issues,
        "rewriteSuggestion": None,
        "encouragement": encouragement
    }

def mock_quick_queries(nodes, edges):
    """Generate follow-up questions in suggestive tone"""
    qs = []
    pn = [n for n in nodes if n.type=="process"]
    dn = [n for n in nodes if n.type=="decision"]

    if not any(n.type=="end" for n in nodes) and len(pn)>=2:
        qs.append("ì–´ë–¤ ìƒí™©ì—ì„œ ì´ í”„ë¡œì„¸ìŠ¤ê°€ ì™„ë£Œë˜ë‚˜ìš”?")

    if not dn and len(pn)>=3:
        qs.append("ì¤‘ê°„ì— íŒë‹¨ì´ë‚˜ ìŠ¹ì¸ì´ í•„ìš”í•œ ì§€ì ì´ ìˆì„ê¹Œìš”?")

    if len(pn)>=2:
        qs.append("ì˜ˆì™¸ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” ìƒí™©ì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆì„ê¹Œìš”?")

    if any(n.systemName for n in nodes):
        qs.append("ì‹œìŠ¤í…œ ê°„ ë°ì´í„° ì—°ê³„ëŠ” ì–´ë–»ê²Œ ì´ë£¨ì–´ì§€ë‚˜ìš”?")

    return qs[:3]

def mock_review(nodes, edges):
    """Mock flow review with encouraging, suggestive tone"""
    suggestions = []
    end_nodes = [n for n in nodes if n.type == 'end']

    if not end_nodes:
        suggestions.append({
            "action": "ADD",
            "type": "END",
            "summary": "ì¢…ë£Œ ë…¸ë“œ ì¶”ê°€",
            "reason": "í”Œë¡œìš°ì˜ ëì„ ëª…í™•íˆ í‘œì‹œí•˜ë©´ ì™„ê²°ì„±ì´ ë†’ì•„ì§‘ë‹ˆë‹¤",
            "reasoning": "í”„ë¡œì„¸ìŠ¤ì˜ ì‹œì‘ê³¼ ëì´ ëª…í™•í•˜ë©´ ì œ3ìê°€ ì „ì²´ ë²”ìœ„ë¥¼ ì´í•´í•˜ê¸° ì‰¬ì›Œì§‘ë‹ˆë‹¤. HR í”„ë¡œì„¸ìŠ¤ì—ì„œëŠ” íŠ¹íˆ ì™„ë£Œ ì¡°ê±´(ì˜ˆ: ê²°ê³¼ ì €ì¥, ì•Œë¦¼)ì„ ëª…ì‹œí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
            "confidence": "high",
            "newLabel": "ì¢…ë£Œ"
        })

    orphans = [n for n in nodes if n.type not in ('start','end') and not any(e.source == n.id or e.target == n.id for e in edges)]
    if orphans:
        suggestions.append({
            "action": "MODIFY",
            "summary": f"ì—°ê²°ë˜ì§€ ì•Šì€ ë…¸ë“œ {len(orphans)}ê°œ ë°œê²¬",
            "reason": "ëª¨ë“  ë‹¨ê³„ë¥¼ ì—°ê²°í•˜ë©´ í”Œë¡œìš°ê°€ ë” ëª…í™•í•´ì§‘ë‹ˆë‹¤",
            "reasoning": f"ë…ë¦½ì ìœ¼ë¡œ ë– ìˆëŠ” ë…¸ë“œëŠ” ì‹¤í–‰ ìˆœì„œê°€ ë¶ˆëª…í™•í•©ë‹ˆë‹¤. ì–´ëŠ ë‹¨ê³„ ì´í›„ì— ìˆ˜í–‰ë˜ëŠ”ì§€, ë˜ëŠ” ë³‘ë ¬ë¡œ ì§„í–‰ë˜ëŠ”ì§€ë¥¼ í‘œí˜„í•˜ë©´ ìš´ì˜ íš¨ìœ¨ì„±ì´ ë†’ì•„ì§‘ë‹ˆë‹¤.",
            "confidence": "high"
        })

    decisions = [n for n in nodes if n.type == 'decision']
    if not decisions and len(nodes) > 5:
        suggestions.append({
            "action": "ADD",
            "type": "DECISION",
            "summary": "ë¶„ê¸°ì  ì¶”ê°€ ê³ ë ¤",
            "reason": "ìŠ¹ì¸/ë°˜ë ¤ ê°™ì€ íŒë‹¨ ì§€ì ì„ ì¶”ê°€í•˜ë©´ ì‹¤ì œ í”„ë¡œì„¸ìŠ¤ì— ë” ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤",
            "reasoning": "HR í”„ë¡œì„¸ìŠ¤ëŠ” ëŒ€ë¶€ë¶„ ì¡°ê±´ë¶€ ë¶„ê¸°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤(ì˜ˆ: ì¡°ê±´ ê²€í†  â†’ ìŠ¹ì¸/ë°˜ë ¤ ê²°ì •). 5ê°œ ì´ìƒì˜ ë‹¨ê³„ê°€ ìˆëŠ”ë° ë¶„ê¸°ê°€ ì—†ë‹¤ë©´, ì˜ˆì™¸ ì²˜ë¦¬ë‚˜ ê²€í†  í”„ë¡œì„¸ìŠ¤ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.",
            "confidence": "medium"
        })

    # Positive framing
    tone = "ê¸ì •ì " if len(suggestions) < 2 else "ê±´ì„¤ì "
    speech = f"ì´ {len(nodes)}ê°œ ë‹¨ê³„ë¡œ ì˜ êµ¬ì„±ë˜ì…¨ë„¤ìš”! " if len(nodes) > 2 else "ì¢‹ì€ ì‹œì‘ì…ë‹ˆë‹¤! "

    if suggestions:
        speech += f"{len(suggestions)}ê°€ì§€ ê°œì„  ì•„ì´ë””ì–´ë¥¼ ê³µìœ ë“œë¦´ê²Œìš”."
    else:
        speech += "êµ¬ì¡°ì ìœ¼ë¡œ íƒ„íƒ„í•©ë‹ˆë‹¤. ì„¸ë¶€ ë‚´ìš©ì„ ë‹¤ë“¬ì–´ê°€ì‹œë©´ ë©ë‹ˆë‹¤!"

    return {
        "speech": speech,
        "suggestions": suggestions,
        "quickQueries": mock_quick_queries(nodes, edges),
        "tone": tone
    }

@app.post("/api/review")
async def review_flow(req: ReviewRequest):
    fd = describe_flow(req.currentNodes, req.currentEdges)
    r = await call_llm(REVIEW_SYSTEM, f"ì»¨í…ìŠ¤íŠ¸: {req.context}\ní”Œë¡œìš°:\n{fd}")
    return r or mock_review(req.currentNodes, req.currentEdges)

@app.post("/api/chat")
async def chat(req: ChatRequest):
    fd = describe_flow(req.currentNodes, req.currentEdges)
    r = await call_llm(COACH_TEMPLATE, f"ì»¨í…ìŠ¤íŠ¸: {req.context}\ní”Œë¡œìš°:\n{fd}\nì§ˆë¬¸: {req.message}")
    return r or {"speech":"AI ì—°ê²° ìƒíƒœê°€ ì›í™œí•˜ì§€ ì•Šì•„ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.","suggestions":[],"quickQueries":[]}

@app.post("/api/validate-l7")
async def validate_l7(req: ValidateL7Request):
    r = await call_llm(L7_VALIDATE, f"ë…¸ë“œ: [{req.nodeId}] {req.nodeType}\nL7: \"{req.label}\"\nì»¨í…ìŠ¤íŠ¸: {req.context}")
    return r or mock_validate(req.label)



@app.post("/api/contextual-suggest")
async def contextual_suggest(req: ContextualSuggestRequest):
    fd = describe_flow(req.currentNodes, req.currentEdges)
    r = await call_llm(CONTEXTUAL_SUGGEST_SYSTEM, f"ì»¨í…ìŠ¤íŠ¸: {req.context}\ní”Œë¡œìš°:\n{fd}")
    return r or {"guidance":"","quickQueries":[]}

@app.post("/api/first-shape-welcome")
async def first_shape_welcome(req: ContextualSuggestRequest):
    process_name = req.context.get("processName", "HR í”„ë¡œì„¸ìŠ¤")
    process_type = req.context.get("l5", "í”„ë¡œì„¸ìŠ¤")
    r = await call_llm(FIRST_SHAPE_SYSTEM, f"í”„ë¡œì„¸ìŠ¤ëª…: {process_name}\ní”„ë¡œì„¸ìŠ¤ íƒ€ì…: {process_type}\n\nì‚¬ìš©ìê°€ ì´ í”„ë¡œì„¸ìŠ¤ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. í™˜ì˜í•˜ê³  ê²©ë ¤í•´ì£¼ì„¸ìš”.")

    if r:
        return {
            "text": f"ğŸ‘‹ {r.get('greeting', '')}\n\n{r.get('processFlowExample', '')}\n\n{r.get('guidanceText', '')}",
            "quickQueries": r.get('quickQueries', [])
        }
    return {
        "text": f"ğŸ‘‹ ì¢‹ì€ ì‹œì‘ì…ë‹ˆë‹¤! \"{process_name}\" í”„ë¡œì„¸ìŠ¤ë¥¼ í•¨ê»˜ ì™„ì„±í•´ë³´ê² ìŠµë‹ˆë‹¤.",
        "quickQueries": ["ì¼ë°˜ì ì¸ ë‹¨ê³„ëŠ” ë­ê°€ ìˆë‚˜ìš”?", "ì–´ë–¤ ë¶„ê¸°ì ì´ í•„ìš”í• ê¹Œìš”?"]
    }

@app.post("/api/analyze-pdd")
async def analyze_pdd(req: ReviewRequest):
    fd = describe_flow(req.currentNodes, req.currentEdges)
    r = await call_llm(PDD_ANALYSIS, f"ì»¨í…ìŠ¤íŠ¸: {req.context}\ní”Œë¡œìš°:\n{fd}")
    if r: return r
    recs = []
    for n in req.currentNodes:
        if n.type in ('start','end'): continue
        cat = 'as_is'
        if any(k in n.label for k in ['ì¡°íšŒ','ì…ë ¥','ì¶”ì¶œ','ì§‘ê³„']): cat = 'digital_worker'
        elif any(k in n.label for k in ['í†µë³´','ì•ˆë‚´','ë°œì†¡']): cat = 'ssc_transfer'
        recs.append({"nodeId":n.id,"nodeLabel":n.label,"suggestedCategory":cat,"reason":"ê·œì¹™ ê¸°ë°˜","confidence":"low"})
    return {"recommendations":recs,"summary":"ê·œì¹™ ê¸°ë°˜ ìë™ ë¶„ë¥˜ì…ë‹ˆë‹¤."}

@app.get("/api/health")
async def health():
    llm = await check_llm()
    return {"status":"ok","version":"5.0","llm_connected":llm,"mode":"live" if llm else "mock"}

if __name__ == "__main__":
    import uvicorn
    try:
        uvicorn.run(app, host="0.0.0.0", port=8000)
    except SystemExit:
        logger.warning("Port 8000 is busy. Trying port 8002...")
        uvicorn.run(app, host="0.0.0.0", port=8002)
